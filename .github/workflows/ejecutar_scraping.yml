name: Ejecutar Scraper Diario

on:
  schedule:
    - cron: '56 17 * * *'   # 17:56 UTC (19:56 Madrid en verano)
  workflow_dispatch:

# Hace que todos los 'run:' se ejecuten dentro de ./scrapping
defaults:
  run:
    shell: bash
    working-directory: scrapping

jobs:
  run-script:
    runs-on: ubuntu-latest

    steps:
      - name: Clonar repositorio
        uses: actions/checkout@v3

      - name: Configurar Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # (Opcional) Cache de pip para acelerar
      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('scrapping/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Instalar dependencias
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Ejecutar script principal
        run: |
          echo "ðŸ“… Ejecutando main.py el $(date -u)"
          python main.py

      - name: Notificar a n8n (webhook)
        if: success()
        env:
          N8N_URL: ${{ secrets.N8N_URL }}
          N8N_WEBHOOK_SECRET: ${{ secrets.N8N_WEBHOOK_SECRET }}
        # Este paso no necesita 'working-directory', pero no afecta dejarlo
        run: |
          curl -sS -i -X POST "$N8N_URL/webhook/scraping-finalizado" \
            -H "Content-Type: application/json" \
            -H "x-n8n-secret: $N8N_WEBHOOK_SECRET" \
            --data @- <<'JSON'
          {
            "status": "ok",
            "mensaje": "âœ… Scraping finalizado correctamente en GitHub Actions",
            "timestamp": "${{ github.run_started_at }}",
            "run_id": "${{ github.run_id }}",
            "workflow": "${{ github.workflow }}"
          }
          JSON
