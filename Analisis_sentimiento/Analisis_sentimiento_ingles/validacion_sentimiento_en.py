# -*- coding: utf-8 -*-
"""Validacion_sentimiento_en.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14DHktrNb_sRDFvyf-aeDzkhdvCng1zlY
"""

import pandas as pd
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import drive

"""## PASO 1: Preparación del Entorno
En este paso, nos conectamos a nuestro entorno de trabajo. Montamos Google Drive para poder acceder a los archivos que contienen nuestros datos de validación y cargamos el modelo de IA que vamos a probar. La configuración de la GPU (si está disponible) nos permite acelerar los cálculos de predicción.
"""

# PASO 1: Montar Google Drive y cargar el modelo
drive.mount('/content/drive')

# Cargar el archivo de validación del drive
ruta_validacion = '/content/drive/MyDrive/Colab Notebooks/df_validacion.csv'

try:
    df_val = pd.read_csv(ruta_validacion)
    print(f"Archivo de validación cargado con {len(df_val)} registros.")
    print(df_val.head())
except FileNotFoundError:
    print("Error: No se encontró el archivo 'df_validacion.csv'.")
    print("Crea un archivo CSV con columnas 'titular' y 'sentimiento_real' para continuar.")
    exit()

model_name = "mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

"""# ## PASO 2: La Máquina de Predicción
Aquí definimos una función que actúa como nuestro "motor" de predicción. Esta función toma un texto (un titular de noticia), lo procesa para que el modelo lo entienda (tokenización), ejecuta la predicción y convierte el resultado en un sentimiento legible (Positivo, Negativo, etc.) y sus respectivas probabilidades.
"""

# PASO 2: Definir la función de predicción
def get_sentiment(text):
    if not isinstance(text, str) or pd.isna(text):
        return 'N/A', np.nan, np.nan, np.nan

    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    inputs = {key: val.to(device) for key, val in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)

    probabilities = torch.softmax(outputs.logits, dim=1)[0].cpu().numpy()

    label_mapping = {
        0: 'Negativo',
        1: 'Neutral',
        2: 'Positivo'
    }

    predicted_class_id = np.argmax(probabilities)
    sentiment = label_mapping[predicted_class_id]

    prob_negative = probabilities[0]
    prob_neutral = probabilities[1]
    prob_positive = probabilities[2]

    return sentiment, prob_positive, prob_negative, prob_neutral

"""## PASO 3: Ejecución de la Prueba
En este paso, ponemos a prueba el modelo. Aplicamos nuestra "máquina de predicción" a cada uno de los titulares en el conjunto de datos de validación. Los resultados (la predicción de sentimiento y las probabilidades) se almacenan en nuevas columnas en el DataFrame.
"""

# PASO 3: Predecir el sentimiento en los datos de validación
print("\nRealizando predicciones en los datos de validación...")
df_val[['sentimiento_predicho', 'probabilidad_positivo', 'probabilidad_negativa', 'probabilidad_neutral']] = \
    df_val['TITULAR'].apply(lambda x: pd.Series(get_sentiment(x)))

"""## PASO 4: Evaluación de los Resultados
Este es el paso crucial. Comparamos las predicciones del modelo con las etiquetas de sentimiento que ya conocíamos (la "verdad fundamental"). Se calculan métricas como la **precisión** (accuracy), que nos dice el porcentaje de aciertos, y se genera un reporte detallado (el "reporte de clasificación") y la "matriz de confusión" para entender dónde acierta y dónde falla el modelo en cada categoría.
"""

# PASO 4: Evaluar el rendimiento del modelo
# Asignar las columnas correctas
y_real = df_val['SENTIMIENTO_RESULTADO']
y_predicho = df_val['sentimiento_predicho']

# Eliminar los registros con predicciones inválidas (N/A)
df_val_filtrado = df_val[df_val['sentimiento_predicho'] != 'N/A']
y_real_filtrado = df_val_filtrado['SENTIMIENTO_RESULTADO']
y_predicho_filtrado = df_val_filtrado['sentimiento_predicho']

# Asegurarse de que las etiquetas en y_real y y_predicho coincidan
# Usar una lista estandarizada de etiquetas para evitar errores
etiquetas = ['Negativo', 'Neutral', 'Positivo']

# Calcular métricas clave
exactitud = accuracy_score(y_real_filtrado, y_predicho_filtrado)
reporte = classification_report(y_real_filtrado, y_predicho_filtrado, labels=etiquetas)
matriz_conf = confusion_matrix(y_real_filtrado, y_predicho_filtrado, labels=etiquetas)

print("\n--- Resultados de la Validación ---")
print(f"Precisión (Accuracy): {exactitud:.4f}")
print("\nReporte de Clasificación:\n", reporte)

"""## PASO 5: Visualización de la Matriz de Confusión
Finalmente, creamos una visualización que nos ayuda a entender el rendimiento del modelo de un solo vistazo. La **matriz de confusión** muestra de forma clara cuántas predicciones fueron correctas (la diagonal) y cuántas veces el modelo confundió una categoría con otra.
"""

# PASO 5: Visualizar la matriz de confusión
print("Generando la matriz de confusión...")
plt.figure(figsize=(8, 6))
sns.heatmap(matriz_conf, annot=True, fmt='d', cmap='Blues', xticklabels=etiquetas, yticklabels=etiquetas)
plt.xlabel('Sentimiento Predicho')
plt.ylabel('Sentimiento Real')
plt.title('Matriz de Confusión')
plt.show()

"""El modelo `mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis` se ha seleccionado por su **alta precisión del 92%** en la validación, lo que demuestra su fiabilidad. Su principal fortaleza es la **identificación perfecta** de noticias positivas y neutrales. La única debilidad es una ligera confusión con los titulares negativos, pero aún así, su rendimiento general es excelente para la tarea de análisis de sentimiento en noticias financieras en inglés."""

